using System.Threading;
using System.Threading.Tasks;
using Honua.Cli.AI.Services.AI;
using Honua.Cli.AI.Services.Agents;
using Honua.Cli.AI.Services.AI.Providers;
using Microsoft.Extensions.Logging.Abstractions;
using Xunit;

namespace Honua.Cli.AI.Tests.Services.Agents;

public sealed class LlmPlanCriticTests
{
    [Fact]
    public async Task EvaluateAsync_ShouldReturnWarningsFromLlm()
    {
        var llm = new StubLlmProvider("[\"Ensure backup is configured\"]");
        var critic = new LlmPlanCritic(llm, NullLogger<LlmPlanCritic>.Instance);

        var request = new ConsultantRequestSnapshot("Automate backups", DryRun: false, Mode: "MultiAgent");
        var result = new AgentCoordinatorResult
        {
            Success = true,
            Response = "Automation completed",
            Steps =
            {
                new AgentStepResult
                {
                    AgentName = "automation",
                    Action = "Backup",
                    Success = true,
                    Message = "Configured backup job",
                    Duration = System.TimeSpan.FromSeconds(2)
                }
            }
        };

        var warnings = await critic.EvaluateAsync(request, result, CancellationToken.None);

        Assert.Contains("Ensure backup is configured", warnings);
    }

    private sealed class StubLlmProvider : ILlmProvider
    {
        private readonly string _response;

        public StubLlmProvider(string response)
        {
            _response = response;
        }

        public string ProviderName => "stub";
        public string DefaultModel => "stub-model";

        public LlmRequest? LastRequest { get; private set; }

        public Task<LlmResponse> CompleteAsync(LlmRequest request, CancellationToken cancellationToken = default)
        {
            LastRequest = request;
            return Task.FromResult(new LlmResponse
            {
                Content = _response,
                Model = DefaultModel,
                Success = true
            });
        }

        public Task<bool> IsAvailableAsync(CancellationToken cancellationToken = default) => Task.FromResult(true);

        public async IAsyncEnumerable<LlmStreamChunk> StreamAsync(LlmRequest request, [System.Runtime.CompilerServices.EnumeratorCancellation] CancellationToken cancellationToken = default)
        {
            var response = await CompleteAsync(request, cancellationToken);
            yield return new LlmStreamChunk
            {
                Content = response.Content,
                IsFinal = true,
                TokenCount = response.TotalTokens
            };
        }

        public Task<IReadOnlyList<string>> ListModelsAsync(CancellationToken cancellationToken = default)
            => Task.FromResult<IReadOnlyList<string>>(new[] { DefaultModel });
    }
}
