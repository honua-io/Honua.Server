# Grafana Tempo Configuration for Honua Process Framework Testing
# This configuration is optimized for local development and testing
# For production, use object storage (S3, GCS, Azure Blob) instead of local storage

server:
  http_listen_port: 3200
  log_level: info
  log_format: logfmt

distributor:
  receivers:
    otlp:
      protocols:
        grpc:
          endpoint: 0.0.0.0:4317
        http:
          endpoint: 0.0.0.0:4318
    zipkin:
      endpoint: 0.0.0.0:9411
    jaeger:
      protocols:
        thrift_http:
          endpoint: 0.0.0.0:14268

ingester:
  max_block_duration: 5m               # Cut blocks after 5 minutes
  max_block_bytes: 10485760            # 10MB max block size
  complete_block_timeout: 15m

compactor:
  compaction:
    block_retention: 24h               # Keep traces for 24 hours (testing)
    compacted_block_retention: 1h
    compaction_window: 1h
    max_compaction_objects: 1000000
    max_block_bytes: 107374182400      # 100GB
    retention_concurrency: 10

storage:
  trace:
    backend: local
    wal:
      path: /var/tempo/wal
      encoding: snappy
    local:
      path: /var/tempo/blocks
    pool:
      max_workers: 100
      queue_depth: 10000

# Query configuration
query_frontend:
  search:
    concurrent_jobs: 100
    max_duration: 0                    # No limit on search duration

# Metrics generator for trace-to-metrics conversion
metrics_generator:
  registry:
    external_labels:
      source: tempo
      cluster: honua-process-testing
  storage:
    path: /var/tempo/generator/wal
    remote_write:
      - url: http://prometheus:9090/api/v1/write
        send_exemplars: true
  traces_storage:
    path: /var/tempo/generator/traces
  processor:
    service_graphs:
      dimensions:
        - http.method
        - http.target
        - http.status_code
      histogram_buckets: [0.1, 0.2, 0.4, 0.8, 1.6, 3.2, 6.4, 12.8]
    span_metrics:
      dimensions:
        - http.method
        - http.target
        - http.status_code
        - service.name
      histogram_buckets: [0.002, 0.004, 0.008, 0.016, 0.032, 0.064, 0.128, 0.256, 0.512, 1.024, 2.048, 4.096, 8.192]

# Overrides for multi-tenancy (single tenant in this case)
overrides:
  defaults:
    metrics_generator:
      processors: [service-graphs, span-metrics]
    max_traces_per_user: 10000
    max_bytes_per_trace: 5000000       # 5MB max trace size
    max_search_duration: 0             # No limit
    ingestion_rate_limit_bytes: 15000000  # 15MB/s per tenant
    ingestion_burst_size_bytes: 20000000  # 20MB burst

# Usage report (disable for privacy)
usage_report:
  reporting_enabled: false
